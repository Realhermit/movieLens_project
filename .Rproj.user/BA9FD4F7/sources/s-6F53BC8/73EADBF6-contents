---
title: "movieLens_edx_project"
author: "Siddhartha Sampath"
date: "6/10/2019"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Executive Summary

The data used was the movieLens dataset provided by the edx team. Two main dataframes were provided, one called `edx` to be used for training and one called `validation` to be used to test predictions against for calculating the RMSE. 

The goal of the project was to predict movie ratings for the validation dataset while minimizing the root mean square error. In order to do this, I tried many different models that I abandoned either because they did not produce the required RMSE or were unable to handle the size of the dataset. The closest one I found was the xgboost library in R that implements a gradeient boosted tree algorithm that is able to handle large data. Ultimately I implemented a linear model that assigned the rating of a movie to the mean of all ratings minus the mean of the ratings grouped by user and movie Id. The minimum RMSE achieved by this model on the validation dataset is 0.8648177

## Methodology

The movie lens was divided into an `edx` (training) and `validation` (test) respectively.
A summary of both dataframes show us what the look like.
```{r , include=FALSE}
load("processed_work_space.RData")
library(tidyverse)
```
```{r edx}
summary(edx)
summary(validation)
dim(edx)
dim(validation)
```
Code was added to ensure that movieIds and userIds present in the test dataset were included in the training dataset. As can be seen the validation dataset is roughly about 10% in size of the main dataset. 

The size of the dataset also makes it difficult to use standard ml models in Rstudio. Therefore the best approach is to fit a linear model that predicts movie ratings based on movieId and userId and calculating the coefficients manually. 

From observing a histogram of ratings by userId and movieId, we see that some movies are rated more than others and some users are more active than others. This motivates the use of regularization towards the model to ensure that movies with few ratings do not artificially show up as high or low rated movies but rather regress towardst the mean.

```{r input_hists, echo=FALSE}
a <- edx %>% group_by(movieId) %>% summarize(n = n())
hist(a$n, xlab="n", col="grey", main="Ratings by movieId")
a <- edx %>% group_by(userId) %>% summarize(n = n()) %>% filter(n<400)
hist(a$n, xlab="n", col="grey", main="Ratings by userId")
```

The model I fit is as follows (ref. Rafael Irizzary ML notes)
$$ Y_{u,i} = \mu + b_i + b_u + \epsilon_{u,i}$$
where the last $\epsilon$ is a random error term. Each of the parameters above can be estimated from the training set, the $\mu$ is the overall mean of all ratings across all users, the $b_i$ is the movie specific mean and $b_u$ the user specific mean. 

With regularization, we penalize the estimates of the $b_i$ (and similarly the $b_u$) by introducing a term $\lambda$ for movies that have a low number of ratings and could thus be outliers instead of a reliable rating.

These estimates are calculated the following way:
$$\hat{b}_i(\lambda) = \frac{1}{\lambda + n_i}\sum_{u=1}^{n_i}(Y_{u,i} - \hat{\mu})$$

To find the best value of lambda that minimizes the expression (ref. Rafael Irizzary ML notes)

$$\frac{1}{N}\sum_{u,i}(y_{u,i} - b_u - b_i - \mu)^2 + \lambda(\sum_{i}b_i^2 = \sum_{u}b_u^2)$$

I further divided the training set ``edx`` into another training set ``train_set`` and a test set ``test_set`` and chose the value of lambda that minimized the RMSE.

```{r lambda, echo=TRUE}
qplot(lambdas, rmses)  
lambda <- lambdas[which.min(rmses)]
print(lambda)
```

This value of 5 was then used to calculate the final model on all of ``edx`` and the estimates calculated were used to predict the ratings for ``validation`` and calculate the final RMSE.

## Results
The final estimates were as follows
```{r results, echo= FALSE}

print(paste0("mu = ", round(mu,3)))
print(paste0("lambda = ", round(lambda,3)))
print(paste0("RMSE = ", round(final_rmse,5)))

```

$b_u$ and $b_i$ were widely distributed as seen below
```{r b_plots, echo=FALSE}
hist(b_i$b_i)
hist(b_u$b_u)
```

These movie and user estimates accordingly adjust the movie rating according to movie and user. 

## Conclusion
The recommendation model used whlie simple is quite accurate and able to scale well with large data. Other models I used such as xgboost (not shown here) while able to handle the large data weren't able to beat this model when it came to minimizing rmse.
Further model enhancements could be done to improve the model by including timestamp and genre effects as well. For example it could be that users became more conservative with their ratings as time passed, or that certain genres are generally better rated than others (is this why very few purely comedic movies win the best film oscar?). Investigations into the effects of modeling these variables may help bring the RMSE of the predictions on the validation set further.